[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Analysis in Medical Research Using R",
    "section": "",
    "text": "Welcome to the Statistical Analysis in Medical Research Using R tutorial series. This series is designed to introduce you to the fundamental concepts of statistical analysis using the R programming language, with a focus on applications in medical research.\n\n\nIn this series, you will learn how to:\n\nUnderstand the importance of statistical analysis in medical research\nPerform basic data manipulation and visualization in R\nApply descriptive and inferential statistics to medical data\nInterpret the results of statistical analyses in the context of medical research\n\n\n\n\nBelow are the chapters included in this series. Each chapter builds on the previous one, so it’s recommended to go through them in order.\n\n\nIn the first chapter, we provide an overview of the role of statistics in medical research. You will learn about different types of data, common statistical terms, and why statistical analysis is crucial in the medical field.\n\n\n\nThe second chapter introduces you to the basics of programming in R. You will learn how to install R and RStudio, understand R syntax, and perform basic operations and data manipulation. This chapter sets the foundation for more advanced topics covered later in the series.\n\n\n\nThis chapter provides an introduction to the fundamental concepts and techniques involved in gene differential expression analysis, focusing on alignment, RNA sequencing, and counting.\n\n\n\n\nTo get started, ensure you have R and RStudio installed on your computer. If not, follow the installation instructions provided in Chapter 02.\n```{r, echo=FALSE} # Sample R code snippet to show in the document cat(“Welcome to the Statistical Analysis in Medical Research Using R!”)",
    "crumbs": [
      "Home",
      "Statistical Analysis in Medical Research Using R"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Statistical Analysis in Medical Research Using R",
    "section": "",
    "text": "In this series, you will learn how to:\n\nUnderstand the importance of statistical analysis in medical research\nPerform basic data manipulation and visualization in R\nApply descriptive and inferential statistics to medical data\nInterpret the results of statistical analyses in the context of medical research",
    "crumbs": [
      "Home",
      "Statistical Analysis in Medical Research Using R"
    ]
  },
  {
    "objectID": "index.html#chapters",
    "href": "index.html#chapters",
    "title": "Statistical Analysis in Medical Research Using R",
    "section": "",
    "text": "Below are the chapters included in this series. Each chapter builds on the previous one, so it’s recommended to go through them in order.\n\n\nIn the first chapter, we provide an overview of the role of statistics in medical research. You will learn about different types of data, common statistical terms, and why statistical analysis is crucial in the medical field.\n\n\n\nThe second chapter introduces you to the basics of programming in R. You will learn how to install R and RStudio, understand R syntax, and perform basic operations and data manipulation. This chapter sets the foundation for more advanced topics covered later in the series.\n\n\n\nThis chapter provides an introduction to the fundamental concepts and techniques involved in gene differential expression analysis, focusing on alignment, RNA sequencing, and counting.",
    "crumbs": [
      "Home",
      "Statistical Analysis in Medical Research Using R"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Statistical Analysis in Medical Research Using R",
    "section": "",
    "text": "To get started, ensure you have R and RStudio installed on your computer. If not, follow the installation instructions provided in Chapter 02.\n```{r, echo=FALSE} # Sample R code snippet to show in the document cat(“Welcome to the Statistical Analysis in Medical Research Using R!”)",
    "crumbs": [
      "Home",
      "Statistical Analysis in Medical Research Using R"
    ]
  },
  {
    "objectID": "Chapter02_BasicProgramming.html",
    "href": "Chapter02_BasicProgramming.html",
    "title": "Basic programing",
    "section": "",
    "text": "This lecture has avoided the term “programming” due to its intimidating connotation. While advanced programming is a specialized skill, the basics are accessible and can achieve impressive results. This chapter introduces basic programming concepts in R, a language you’ve been using interactively so far.\nProgramming is essentially writing instructions for a computer in a language it can interpret, as a cooking recipe in a language like R. You’ve been entering commands at the prompt, but you can also write a program using R commands. This means you’re close to being able to program in R at a beginner’s level.\nComputer programs can take many forms, including scripts, which are of interest for everyday data analysis in R. Scripts involve writing all commands in a text file, which R can execute using the source() function.\nScripts are useful for several reasons. They allow you to save your work effectively, making it easier to correct mistakes without starting from scratch. They also let you leave notes for yourself, aiding recall of your analysis process. Finally, scripts make it easier to reuse or adapt your analyses for similar problems in the future.\nThe interactive style of entering commands one at a time has its uses, but also drawbacks. It’s challenging to save your work effectively, tedious to correct mistakes, and difficult to leave notes or reuse analyses. Scripts address these issues, making them a valuable tool for data analysis.",
    "crumbs": [
      "Introduction",
      "Basic programing"
    ]
  },
  {
    "objectID": "Chapter02_BasicProgramming.html#flow-control",
    "href": "Chapter02_BasicProgramming.html#flow-control",
    "title": "Basic programing",
    "section": "Flow control",
    "text": "Flow control\nIn scripting, R doesn’t necessarily start at the top of the file and run straight through to the end. Depending on how you write the script, you can have R repeat several commands or skip over different commands. This topic is referred to as flow control, and the first concept to discuss in this respect is the idea of a loop. A loop in R is a control flow statement that allows you to repeat a block of code multiple times. There are several types of loops in R, each serving different purposes and use cases. The most common types of loops in R are for loops and while loops.\nA loop in R is a control flow statement that allows you to repeat a block of code multiple times. There are several types of loops in R, each serving different purposes and use cases. The most common types of loops in R are for loops and while loops.\n\nfor Loop\nA for loop is used to iterate over a sequence (such as a vector, list, or any iterable object) and execute a block of code for each element in the sequence.\n\nSyntax:\n$ R\nfor (variable in sequence) {\n  # Code to be executed for each element\n}\n\n\nExample:\n\n# Print numbers from 1 to 5\nfor (i in 1:5) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\n\n\n\nwhile Loop\nA while loop repeatedly executes a block of code as long as a specified condition is TRUE.\n\nSyntax:\n$ R\nwhile (condition) {\n  # Code to be executed as long as the condition is TRUE\n}\n\n\nExample:\n\n# Print numbers from 1 to 5\ni &lt;- 1\nwhile (i &lt;= 5) {\n  print(i)\n  i &lt;- i + 1\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\n\n\n\nrepeat Loop\nA repeat loop executes an infinite loop unless explicitly stopped using the break statement.\n\nSyntax:\n$ R\nrepeat {\n  # Code to be executed repeatedly\n  if (condition) {\n    break\n  }\n}\n\n\nExample:\n\n# Print numbers from 1 to 5\ni &lt;- 1\nrepeat {\n  print(i)\n  i &lt;- i + 1\n  if (i &gt; 5) {\n    break\n  }\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\n\n\n\nBreaking Out of Loops\nYou can use the break statement to exit a loop prematurely, and the next statement to skip the current iteration and move to the next iteration of the loop.\n\nbreak Example:\n\nfor (i in 1:10) {\n  if (i == 6) {\n    break\n  }\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\n\n\nnext Example:\n\nfor (i in 1:10) {\n  if (i %% 2 == 0) {\n    next\n  }\n  print(i)\n}\n\n[1] 1\n[1] 3\n[1] 5\n[1] 7\n[1] 9\n\n\n\n\n\nLoop Alternatives\nIn R, vectorized operations and the apply family of functions (lapply, sapply, apply, tapply, mapply) are often used as alternatives to loops for more efficient and concise code.\n\nExample using lapply:\n\n# Create a list of numbers from 1 to 5\nnumbers &lt;- list(1, 2, 3, 4, 5)\n\n# Apply a function to each element of the list\nsquared_numbers &lt;- lapply(numbers, function(x) x^2)\n\n# Print the squared numbers\nprint(squared_numbers)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 9\n\n[[4]]\n[1] 16\n\n[[5]]\n[1] 25\n\n\nLoops are powerful tools in R for repeating tasks and iterating over data structures. However, for performance and readability, vectorized operations and apply functions are often preferred.\nLet’s use a bioinformatics example. Suppose we have a vector of DNA sequences and we want to calculate the GC content of each sequence. Here’s the script:\n\n## --- bioexample.R\n#  author: Andres Chamorro\n#  date: Jun 26 2024\n\n# The DNA sequences\nsequences &lt;- c(\"ATGCGTA\", \"CGTACGT\", \"TACGTACG\")\n\n# Loop over the sequences\nfor (seq in sequences) {\n\n  # Calculate the GC content\n  gc_content &lt;- (sum(nchar(gsub(\"[AT]\", \"\", seq))) / nchar(seq)) * 100\n  \n  # Print the GC content\n  print(paste(\"The GC content of\", seq, \"is\", round(gc_content, 2), \"%\"))\n  \n}\n\n[1] \"The GC content of ATGCGTA is 42.86 %\"\n[1] \"The GC content of CGTACGT is 57.14 %\"\n[1] \"The GC content of TACGTACG is 50 %\"\n\n\nIn this script, the vector of possible values for the seq variable corresponds to the DNA sequences. The body of the loop calculates the GC content of each sequence and prints it. When we run this script, R starts at the top and creates a new variable called seq and assigns it a value of the first DNA sequence. It then moves down to the loop, and “notices” that there are more sequences in the vector. It then enters the body of the loop (inside the curly braces). The commands here instruct R to calculate the GC content of the sequence and print it. R then returns to the top of the loop, and rechecks if there are more sequences in the vector. If there are, then R goes on to execute all … well, you get the idea. This continues until all sequences in the vector have been processed. At this point, the loop stops, and R finally reaches the end of the script.\nConditional statements in R allow you to execute different pieces of code based on whether a condition is true or false. The most common conditional statements in R are if, else, and else if.\n\n\n\nConditional statements\nThe if statement allows you to execute a block of code if a condition is true.\n\nSyntax:\n$ R\nif (condition) {\n  # Code to execute if condition is true\n}\n\n\nExample:\n\nx &lt;- 5\n\nif (x &gt; 0) {\n  print(\"x is positive\")\n}\n\n[1] \"x is positive\"\n\n\n\n\nif-else Statement\nThe if-else statement allows you to execute one block of code if a condition is true and another block of code if the condition is false.\n\n\nSyntax:\n$ R\nif (condition) {\n  # Code to execute if condition is true\n} else {\n  # Code to execute if condition is false\n}\n\n\nExample:\n\nx &lt;- -3\n\nif (x &gt; 0) {\n  print(\"x is positive\")\n} else {\n  print(\"x is negative or zero\")\n}\n\n[1] \"x is negative or zero\"\n\n\nThe if-else statement allows you to check multiple conditions and execute different blocks of code based on which condition is true.\n\n\nExample:\n\nx &lt;- 0\n\nif (x &gt; 0) {\n  print(\"x is positive\")\n} else if (x &lt; 0) {\n  print(\"x is negative\")\n} else {\n  print(\"x is zero\")\n}\n\n[1] \"x is zero\"\n\n\nConditional statements are essential in R for decision-making and control flow, allowing you to execute different code paths based on specific conditions.",
    "crumbs": [
      "Introduction",
      "Basic programing"
    ]
  },
  {
    "objectID": "Chapter02_BasicProgramming.html#functions-in-r",
    "href": "Chapter02_BasicProgramming.html#functions-in-r",
    "title": "Basic programing",
    "section": "Functions in R",
    "text": "Functions in R\nA function in R is a block of code that performs a specific task, which can be reused and executed when called. Functions can take inputs (arguments), perform operations on those inputs, and return outputs.\n\nDefining Functions\n\nSyntax:\n$ R\nfunction_name &lt;- function(arg1, arg2, ...) {\n  # Code to execute\n  return(result)\n}\n\n\nExample:\n\n# Define a simple function to add two numbers\nadd_numbers &lt;- function(a, b) {\n  sum &lt;- a + b\n  return(sum)\n}\n\n# Call the function\nresult &lt;- add_numbers(3, 5)\nprint(result)  # Output: 8\n\n[1] 8\n\n\n\n\n\nCustom Functions\nFunctions in R are powerful tools that can be customized to perform a wide variety of tasks. In bioinformatics, they are particularly useful for tasks such as sequence analysis, gene expression normalization, and sequence alignment. By encapsulating code into functions, you can make your analysis more atomic, reusable, and easier to understand.\n\n1. DNA Sequence Analysis\n\nExample: Function to Count Nucleotides\nCounting nucleotides is a fundamental task in genomics. Understanding the composition of a DNA sequence can provide insights into genetic variations and functions.\n\n# Function to count nucleotides in a DNA sequence\ncount_nucleotides &lt;- function(dna_sequence) {\n  counts &lt;- table(strsplit(dna_sequence, \"\")[[1]])\n  return(counts)\n}\n\n# Test the function\ndna &lt;- \"ATGCTAGCTAGGCTA\"\nnucleotide_counts &lt;- count_nucleotides(dna)\nprint(nucleotide_counts)  # Output: A C G T \n\n\nA C G T \n4 3 4 4 \n\n                          #         4 4 3 4\n\n\n\n\n2. Protein Sequence Analysis\n\nExample: Function to Translate DNA to Protein\nTranslating DNA to protein is crucial for understanding gene expression and function. This function helps in converting nucleotide sequences into their corresponding amino acid sequences, aiding in protein analysis.\n\n# Load necessary library\n# To install package, enter in console\"\n# if (!require(\"BiocManager\", quietly = TRUE))\n#    install.packages(\"BiocManager\")\n# BiocManager::install(\"Biostrings\")\nlibrary(Biostrings)\n\nLoading required package: BiocGenerics\n\n\n\nAttaching package: 'BiocGenerics'\n\n\nThe following objects are masked from 'package:stats':\n\n    IQR, mad, sd, var, xtabs\n\n\nThe following objects are masked from 'package:base':\n\n    anyDuplicated, aperm, append, as.data.frame, basename, cbind,\n    colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,\n    get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,\n    match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,\n    Position, rank, rbind, Reduce, rownames, sapply, setdiff, table,\n    tapply, union, unique, unsplit, which.max, which.min\n\n\nLoading required package: S4Vectors\n\n\nLoading required package: stats4\n\n\n\nAttaching package: 'S4Vectors'\n\n\nThe following object is masked from 'package:utils':\n\n    findMatches\n\n\nThe following objects are masked from 'package:base':\n\n    expand.grid, I, unname\n\n\nLoading required package: IRanges\n\n\nLoading required package: XVector\n\n\nLoading required package: GenomeInfoDb\n\n\n\nAttaching package: 'Biostrings'\n\n\nThe following object is masked from 'package:base':\n\n    strsplit\n\n# Function to translate DNA sequence to protein sequence\ntranslate_dna &lt;- function(dna_sequence) {\n  protein_sequence &lt;- sapply(seq(1, nchar(dna_sequence) - 2, by = 3), function(i) {\n    codon &lt;- substr(dna_sequence, i, i + 2)\n    return(GENETIC_CODE[[codon]])\n  })\n\n  return(paste(protein_sequence, collapse = \"\"))\n}\n\n# Test the function\ndna &lt;- \"ATGGTCTAACGTA\"\nprotein &lt;- translate_dna(dna)\nprint(protein)  # Output: \"MV*R\"\n\n[1] \"MV*R\"\n\n\n\n\n\n3. Sequence Alignment\n\nExample: Function to Perform Global Alignment\nSequence alignment is a core bioinformatics task used to find similarities between sequences, which can indicate functional, structural, or evolutionary relationships. The pairwiseAlignment function from the Biostrings package in Bioconductor provides a robust way to perform sequence alignments.\n\n# Load necessary library\n# To install package, enter in console\"\n# if (!require(\"BiocManager\", quietly = TRUE))\n#    install.packages(\"BiocManager\")\n# BiocManager::install(\"pwalign\")\nlibrary(pwalign)\n\n\nAttaching package: 'pwalign'\n\n\nThe following objects are masked from 'package:Biostrings':\n\n    aligned, alignedPattern, alignedSubject, compareStrings, deletion,\n    errorSubstitutionMatrices, indel, insertion, mismatchSummary,\n    mismatchTable, nedit, nindel, nucleotideSubstitutionMatrix,\n    pairwiseAlignment, PairwiseAlignments,\n    PairwiseAlignmentsSingleSubject, pid, qualitySubstitutionMatrices,\n    stringDist, unaligned, writePairwiseAlignments\n\n# Function to perform global alignment between two sequences\nglobal_alignment &lt;- function(seq1, seq2) {\n  alignment &lt;- pairwiseAlignment(seq1, seq2, type = \"global\")\n  return(alignment)\n}\n\n# Test the function\nseq1 &lt;- DNAString(\"ATGCTAGCTAG\")\nseq2 &lt;- DNAString(\"ATGCTAGGCTA\")\nalignment_result &lt;- global_alignment(seq1, seq2)\nprint(alignment_result)\n\nGlobal PairwiseAlignmentsSingleSubject (1 of 1)\npattern: ATGCTAG-CTAG\nsubject: ATGCTAGGCTA-\nscore: -8.182438 \n\n\n\n\n\n4. Gene Expression Analysis\n\nExample: Function to Normalize Gene Expression Data\nNormalizing gene expression data is essential for comparing expression levels across different genes and samples. Log transformation is a common normalization method to reduce skewness and stabilize variance.\n\n# Function to normalize gene expression data using log transformation\nnormalize_expression &lt;- function(expression_data) {\n  normalized_data &lt;- log2(expression_data + 1)\n  return(normalized_data)\n}\n\n# Test the function\nexpression_data &lt;- c(100, 200, 300, 400, 500)\nnormalized_data &lt;- normalize_expression(expression_data)\nprint(normalized_data)  # Output: 6.658211 7.643856 8.228819 8.643856 8.965784\n\n[1] 6.658211 7.651052 8.233620 8.647458 8.968667",
    "crumbs": [
      "Introduction",
      "Basic programing"
    ]
  },
  {
    "objectID": "Chapter02_BasicProgramming.html#exercises",
    "href": "Chapter02_BasicProgramming.html#exercises",
    "title": "Basic programing",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Analyzing Gene Expression Data\n\nObjective:\nWrite a function in R to calculate basic descriptive statistics (mean, median, standard deviation) for a given gene expression dataset.\n\n\nInstructions:\n\nCreate a random vector gene_expression, of size 100 with uniform distribution.\nWrite a function describe_expression that takes a numeric vector as input and returns a list with the mean, median, and standard deviation of the input data.\n\n\n\n\nExercise 2: Descriptive Statistics of Protein Abundances\n\nObjective:\nAnalyze the protein abundance data from different samples and calculate descriptive statistics for each sample.\n\n\nInstructions:\n\nCreate a data frame protein_data with the following values:\n\nSample1: c(0.5, 0.8, 1.0, 1.2, 0.9)\nSample2: c(1.5, 1.7, 1.6, 1.8, 1.9)\nSample3: c(2.1, 2.3, 2.2, 2.5, 2.4)\n\nWrite a function summary_statistics that calculates the mean, median, and standard deviation for each sample in the data frame.\nTest the function with the protein_data data frame.\n\n\n\nTemplate:\n\n# Create the data frame with protein abundance data\nprotein_data &lt;- data.frame(\n  dummy = double()\n  # Code here\n)\n\n# Define the function to calculate descriptive statistics for each numeric column in a data frame\nsummary_statistics &lt;- function(data) {\n  # Check if input is a data frame\n  if (!is.data.frame(data)) {\n    stop(\"Input must be a data frame\")\n  }\n  \n  # Initialize an empty list to store statistics for each column\n  stats_list &lt;- list()\n  \n  # Iterate over each column in the data frame\n  for (colname in names(data)) {\n    # Check if the column is numeric\n    if (is.numeric(data[[colname]])) {\n      # Compute descriptive statistics on column colname\n      mean_value = 0.0 # Dummy value\n      median_value = 0.0 # Dummy value\n      sd_value = 0.0 # Dummy value\n      # Example\n      min_value &lt;- min(data[[colname]], na.rm = TRUE)\n      max_value &lt;- max(data[[colname]], na.rm = TRUE)\n      \n      # Create a named vector of statistics\n      stats &lt;- c(mean = mean_value, median = median_value, sd = sd_value, min = min_value, max = max_value)\n      # Add the statistics to the list with the column name as the key\n      stats_list[[colname]] &lt;- stats\n    }\n  }\n  # Convert the list of statistics to a data frame\n  stats_df &lt;- do.call(rbind, stats_list)\n  # Return the data frame of descriptive statistics\n  return(as.data.frame(stats_df))\n}\n\n# Test the function\nprotein_stats &lt;- summary_statistics(protein_data)\n\nWarning in min(data[[colname]], na.rm = TRUE): no non-missing arguments to min;\nreturning Inf\n\n\nWarning in max(data[[colname]], na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\nknitr::kable(protein_stats)\n\n\n\n\n\nmean\nmedian\nsd\nmin\nmax\n\n\n\n\ndummy\n0\n0\n0\nInf\n-Inf\n\n\n\n\n\n\n\nExpected Output:\n\n\n\n\nmean\nmedian\nsd\nmin\nmax\n\n\n\n\nSample1\n0.88\n0.9\n0.2588436\n0.5\n1.2\n\n\nSample2\n1.70\n1.7\n0.1581139\n1.5\n1.9\n\n\nSample3\n2.30\n2.3\n0.1581139\n2.1\n2.5",
    "crumbs": [
      "Introduction",
      "Basic programing"
    ]
  },
  {
    "objectID": "Chapter01_Introduction.html",
    "href": "Chapter01_Introduction.html",
    "title": "R Basics",
    "section": "",
    "text": "This section covers the basics you need to perform descriptive statistics analysis using R(2024). Similar to other programming languages, R has a base package and an Integrated Development Environment (IDE). The base package allows you to run R code on your computer, while R Studio is an IDE specifically designed for developing R programs and packages.\n\n\nThe R base package can be downloaded from the official R website. Once on the website, select the precompiled binary for your operating system, download the file, and install it. To verify that R has been successfully installed, open your command prompt (cmd) or terminal and type R to start the R console. To exit the R console, type q().\n$ R\n\nR version 4.4.0 (2024-04-24) -- \"Puppy Cup\"\nCopyright (C) 2024 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n&gt; q()\n\n\n\nRStudio is an essential tool for anyone working with the R programming language. It serves as an integrated development environment (IDE) that makes working with R more efficient and user-friendly. Here’s what you need to know:\n\n\nRStudio is a flexible, multifunctional open-source IDE that serves as a graphical front-end for working with R (version 3.0.1 or higher). Additionally, it supports other programming languages like Python and SQL. Key features of RStudio include a user-friendly interface, the ability to write and save reusable scripts, and easy access to imported data and created objects such as variables and functions. It provides exhaustive help on any object, offers code autocompletion, and facilitates project organization and collaboration. Users can preview plots, switch between the terminal and console, and track their operational history, making it an indispensable tool for data analysis and programming.\n\n\n\nTo install RStudio, begin by visiting the official RStudio website. Scroll down to locate the download buttons for RStudio Desktop. Select the precompiled binary appropriate for your operating system, download the file, and install it.\n\n\n\n\n\nDataCamp’s RStudio Tutorial: A comprehensive guide for beginners.\nDataquest’s Getting Started with R and RStudio: Learn key features and start programming in R.\nGitHub Pages: Introduction to RStudio: Fundamentals of RStudio for scientific projects.\nIntroduction to R and RStudio (GitBook): Best practices for organizing code using RStudio.\nRStudio for the Total Beginner: An accessible introduction to RStudio for the total beginner.\n\n\n\n\nLearning Statistics with R\nStatistical Methods in Medical Research\n\n\n\n\n\nDescriptive statistics is a branch of statistics that focuses on summarizing and describing the features of a dataset. It provides simple summaries about the sample and the measures, offering a way to present data in a meaningful and understandable form. The origins of descriptive statistics date back to the early days of statistical science, where it served as the foundational method for analyzing data.\nHistory:\nThe roots of descriptive statistics can be traced back to ancient civilizations, such as the Babylonians and Egyptians, who used basic statistical methods to manage agricultural data, censuses, and astronomical information. However, the formal development of descriptive statistics began in the 17th and 18th centuries. John Graunt, an English demographer, is often credited with laying the groundwork for statistical analysis through his work on mortality rates in the 1660s. His pioneering efforts in collecting and analyzing data led to the birth of modern statistics.\nIn the 18th century, the field was further advanced by the work of mathematicians like Pierre-Simon Laplace and Carl Friedrich Gauss, who developed key statistical concepts and methods. The 19th and 20th centuries saw the emergence of more sophisticated statistical techniques and the formalization of the discipline. Descriptive statistics evolved to include a variety of measures and graphical representations that remain fundamental to data analysis today.\n\nMeasures of Central Tendency:\n\nMean: The arithmetic average of a set of numbers. For example, in a dataset of test scores (80, 85, 90, 75, 95), the mean score is calculated as (80 + 85 + 90 + 75 + 95) / 5 = 85.\nMedian: The middle value in a dataset when the numbers are arranged in ascending order. In the test scores example, the median score is 85.\nMode: The most frequently occurring value in a dataset. If a dataset of test scores is (80, 85, 85, 90, 95), the mode is 85.\n\nMeasures of Dispersion:\n\nRange: The difference between the highest and lowest values in a dataset. For test scores (75, 80, 85, 90, 95), the range is 95 - 75 = 20.\nVariance: A measure of the spread of a dataset. It is the average of the squared differences from the mean. For the test scores, the variance can be calculated by finding the mean, subtracting each score from the mean, squaring the result, and then averaging those squared differences.\nStandard Deviation: The square root of the variance, representing the average amount each value in the dataset differs from the mean. It provides insight into the dataset’s overall variability.\n\nGraphical Representations:\n\nHistograms: Bar graphs that represent the frequency distribution of a dataset. They help in visualizing the shape and spread of data.\nBox Plots: Graphical representations that show the distribution of a dataset based on a five-number summary: minimum, first quartile, median, third quartile, and maximum. Box plots highlight the central tendency and variability, as well as potential outliers.\nPie Charts: Circular charts divided into sectors representing proportions of the whole. They are useful for displaying categorical data and comparing parts of a whole.\n\n\nDescriptive statistics is essential in various fields, including economics, psychology, and social sciences, where it aids in making data-driven decisions. By providing a clear summary of data through measures of central tendency, dispersion, and graphical representations, descriptive statistics helps researchers and analysts interpret and communicate their findings effectively.\n\n\nTo perform descriptive statistics, we need to assign some list of numbers to a variable. This can be done using the `c()` function, which stands for combine.\n\nmy_numbers &lt;- c(1,5,3,2)\n\nThere a few other handy ways to make numbers. We can use seq() to make a sequence. Here’s making the numbers from 1 to 100\n\none_to_one_hundred &lt;- seq(1,100,1)\n\nWe can repeat things, using rep. Here’s making 5 10s, and 25 1s:\n\nrep(10,5)\n\n[1] 10 10 10 10 10\n\nrep(1,25)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\nall_together_now &lt;- c(rep(10,5),rep(1,25)) \n\n\n\n\nLet’s work with the numbers 1 to 256. First, we’ll use the `sum()` function to add them up.\n\none_to_one_hundred &lt;- seq(1,100,1)\nsum(one_to_one_hundred)\n\n[1] 5050\n\n\n\n\n\nWe put 100 numbers into the variable one_to_one_hundred. We know how many numbers there are in there. How can we get R to tell us? We use length() for that.\n\nlength(one_to_one_hundred)\n\n[1] 100\n\n\n\n\n\n\n\nRemember the mean of some numbers is their sum, divided by the number of numbers. We can compute the mean like this:\n\nsum(one_to_one_hundred)/length(one_to_one_hundred)\n\n[1] 50.5\n\n\nOr, we could just use the mean() function like this:\n\nmean(one_to_one_hundred)\n\n[1] 50.5\n\n\n\n\n\nThe median is the number that lies exactly in the middle of a sorted list of numbers. If the list has an even number of elements, the median is calculated as the average of the two middle numbers.\n\nIf \\(n\\) is odd, \\(med(X) = X_{(n+1)/2}\\)\nIf \\(n\\) is even, \\(med(X) = \\frac{X_{n/2} + X_{(n+2) + 1}}{2}\\)\n\nYou can use the median() function to find it. For example, in a list of three numbers, the middle number is 2, so the median is 2.\n\nmedian(c(1,2,3))\n\n[1] 2\n\n\n\n\n\nR does not have a built-in function for calculating the mode. You will need to write one yourself. Here is an example of how to create a mode function and use it. Remember, the mode is the number that appears most frequently in a dataset. In the example below, 1 occurs the most often, so the mode is 1.\n\nmode &lt;- function(x) {\n  ux &lt;- unique(x)\n  ux[which.max(tabulate(match(x, ux)))]\n}\n\nmode(c(1,1,1,1,1,1,1,2,3,4))\n\n[1] 1\n\n\n\n\n\n\nWe often want to understand how much the numbers vary. To describe this variability, we look at descriptive statistics such as range, variance, the standard deviation, and others.\nFirst, let’s remind ourselves what variation looks like (it refers to differences among numbers). We’ll sample 1000 numbers from a normal distribution with a mean of 101 and a standard deviation of 35. Then, we’ll create a histogram to visualize the variation around the mean of 10.\n\nsample_numbers &lt;- rnorm(1000,101,35)\nhist(sample_numbers)\n\n\n\n\n\n\n\n\n\n\nThe range is the minimum and maximum values in the set, we use the range function.\n\nrange(sample_numbers)\n\n[1]  -7.461576 207.119929\n\n\n\n\n\nWe can find the sample variance using var().\n\nvar(sample_numbers)\n\n[1] 1296.951\n\n\n\n\n\nWe find the sample standard deviation us sd().\n\nsd(sample_numbers)\n\n[1] 36.01321\n\n\nRemember that the standard deviation is just the square root of the variance, see:\n\nsqrt(var(sample_numbers))\n\n[1] 36.01321\n\n\n\n\n\n\nsample_numbers &lt;- rnorm(1000,101,35)\n\nsum(sample_numbers)\n\n[1] 101370.6\n\nlength(sample_numbers)\n\n[1] 1000\n\nmean(sample_numbers)\n\n[1] 101.3706\n\nmedian(sample_numbers)\n\n[1] 101.2754\n\nmode(sample_numbers)\n\n[1] 48.96654\n\nrange(sample_numbers)\n\n[1] -42.53119 227.15131\n\nvar(sample_numbers)\n\n[1] 1286.185\n\nsd(sample_numbers)\n\n[1] 35.86342\n\n\n\n\n\n\nIn some instances, a single variable may contain a number of values, which can be analyzed using the aforementioned functions to obtain descriptive statistics. In contrast, in the majority of cases encountered in this course, a data frame comprising multiple numerical values representing distinct conditions will be utilized. In such instances, it is imperative to identify descriptive statistics for each set of values within each condition.\nFortunately, the R programming language is highly adept at performing this task in a single operation. To illustrate this concept, consider the following example. A data frame containing 10 numbers for each condition will be created. There are 10 conditions, each labeled A, B, C, D, E, F, G, H, I, and J.\n\nscores &lt;- rnorm(100,10,5)\nconditions &lt;- rep(c(\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\"), each =10)\nmy_df &lt;- data.frame(conditions,scores)\n\nA review of the my_df data frame reveals that it contains 100 rows. Each row represents a distinct condition, with a label in the conditions column and 10 scores for that condition in the scores column. One might inquire as to the mean of the scores in each condition. In order to obtain the mean of the scores for each condition, one must find the mean of 10 scores.\nThe slow way to do it would be like this:\n\nmean(my_df[my_df$conditions==\"A\",]$scores)\n\n[1] 8.749205\n\nmean(my_df[my_df$conditions==\"B\",]$scores)\n\n[1] 10.85871\n\nmean(my_df[my_df$conditions==\"C\",]$scores)\n\n[1] 7.650824\n\n# and then keep going\n\nIt is evident that no individual or entity is willing to assume the responsibility of performing this task. It is therefore prudent to encapsulate this functionality, and R provides us with the necessary tools to automate this functionality.\n\n\nWe can easily do everything all at once using the group_by and summarise function from the dplyr package. Just watch\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nmy_df %&gt;%\n  group_by(conditions) %&gt;%\n  summarise(means = mean(scores))\n\n# A tibble: 10 × 2\n   conditions means\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 A           8.75\n 2 B          10.9 \n 3 C           7.65\n 4 D          10.0 \n 5 E          12.2 \n 6 F           9.27\n 7 G          11.3 \n 8 H          10.2 \n 9 I           8.76\n10 J          12.4 \n\n\nA few points require further consideration. Firstly, the printout of this was of an inferior quality. A solution to this issue is to create a new variable containing the results of the code and then use the knitr::kable function to print the variable in a more aesthetically pleasing manner when the document is compiled.\n\nsummary_df &lt;- my_df %&gt;%\n               group_by(conditions) %&gt;%\n               summarise(means = mean(scores))\n\nknitr::kable(summary_df)\n\n\n\n\nconditions\nmeans\n\n\n\n\nA\n8.749205\n\n\nB\n10.858707\n\n\nC\n7.650824\n\n\nD\n10.022154\n\n\nE\n12.234878\n\n\nF\n9.266721\n\n\nG\n11.306428\n\n\nH\n10.228266\n\n\nI\n8.759115\n\n\nJ\n12.424293\n\n\n\n\n\n\n\n\nThe most advantageous aspect of the dplyr method is that it permits the addition of multiple functions, resulting in the generation of multiple summary statistics in a unified format. To illustrate this, consider the calculation of the standard deviation:\n\nsummary_df &lt;- my_df %&gt;%\n               group_by(conditions) %&gt;%\n               summarise(means = mean(scores),\n                         sds = sd(scores))\n\nknitr::kable(summary_df)\n\n\n\n\nconditions\nmeans\nsds\n\n\n\n\nA\n8.749205\n3.809475\n\n\nB\n10.858707\n4.655481\n\n\nC\n7.650824\n4.776866\n\n\nD\n10.022154\n5.564239\n\n\nE\n12.234878\n3.934642\n\n\nF\n9.266721\n4.074786\n\n\nG\n11.306428\n6.359828\n\n\nH\n10.228266\n5.714836\n\n\nI\n8.759115\n7.025043\n\n\nJ\n12.424293\n6.270404\n\n\n\n\n\nFurthermore, the minimum and maximum values will be included.\n\nsummary_df &lt;- my_df %&gt;%\n               group_by(conditions) %&gt;%\n               summarise(means = mean(scores),\n                         sds = sd(scores),\n                         min = min(scores),\n                         max = max(scores))\n\nknitr::kable(summary_df)\n\n\n\n\nconditions\nmeans\nsds\nmin\nmax\n\n\n\n\nA\n8.749205\n3.809475\n2.6909396\n14.36260\n\n\nB\n10.858707\n4.655481\n4.3610864\n18.34482\n\n\nC\n7.650824\n4.776866\n1.2825037\n19.33234\n\n\nD\n10.022154\n5.564239\n3.2911257\n17.71844\n\n\nE\n12.234878\n3.934642\n6.2258202\n18.29770\n\n\nF\n9.266721\n4.074786\n3.3725043\n17.87850\n\n\nG\n11.306428\n6.359828\n2.4939802\n23.04869\n\n\nH\n10.228266\n5.714836\n0.0305871\n17.12949\n\n\nI\n8.759115\n7.025043\n-4.1064345\n18.98884\n\n\nJ\n12.424293\n6.270404\n1.8025658\n20.80197\n\n\n\n\n\n\n\n\n\nHaving established the methodology for obtaining descriptive statistics from R, we may now proceed to apply this methodology to a real data set. We will now proceed to inquire about the gapminder data.\n\nlibrary(gapminder)\ngapminder_df &lt;- gapminder\n\nNote: The code will only function properly if the gapminder package has been installed. It is imperative that the user is connected to the internet prior to commencing the installation process. To do so, navigate to the Packages tab, located in the bottom right panel, and select the option to install. Subsequently, a search for the gapminder package should be conducted, after which it should be selected and installed.\n\n\nCopy the code from the last part of descriptives using dplyr, then change the names like this:\n\nsummary_df &lt;- gapminder_df %&gt;%\n               group_by(continent) %&gt;%\n               summarise(means = mean(lifeExp),\n                         sds = sd(lifeExp),\n                         min = min(lifeExp),\n                         max = max(lifeExp))\n\nknitr::kable(summary_df)\n\n\n\n\ncontinent\nmeans\nsds\nmin\nmax\n\n\n\n\nAfrica\n48.86533\n9.150210\n23.599\n76.442\n\n\nAmericas\n64.65874\n9.345088\n37.579\n80.653\n\n\nAsia\n60.06490\n11.864532\n28.801\n82.603\n\n\nEurope\n71.90369\n5.433178\n43.585\n81.757\n\n\nOceania\n74.32621\n3.795611\n69.120\n81.235\n\n\n\n\n\n\n\n\n\nComplete the generalization exercise described in your R Markdown document for this lab.\n\nWhat is the mean, standard deviation, minimum and maximum life expectancy for all the gapminder data (across all the years and countries). Hint: do not use group_by\nWhat is the mean, standard deviation, minimum and maximum life expectancy for all of the continents in 2007, the most recent year in the dataset. Hint: add another pipe using filter(year==2007) %&gt;%\n\n\n\n\nComplete the writing assignment described in your R Markdown document for this lab. When you have finished everything. Knit the document and hand in your stuff (you can submit your .RMD file to blackboard if it does not knit.)\nYour writing assignment is to answer these questions in full sentences using simple plain langauge:\n\nDefine the mode.\nExplain what would need to happen in order for a set of numbers to have two modes\nDefine the median\nDefine the mean\nDefine the range\nWhen calculating the standard deviation, explain what the difference scores represent\nExplain why the difference scores are squared when calculating the standard deviation\nIf one set of numbers had a standard deviation of 5, and another had a standard deviation of 10, which set of numbers would have greater variance, explain why.\n\n\n\n\nNow let’s use a real dataset to calculate the same measures of central tendency and variability as in the previous example, but with the addition of a histogram to visualize the distribution and relate back to the descriptive statistics. Here is a link to the life expectancy dataset we used for our graphing tutorial. It is named life_expectancy.csv.\nSuppose we wanted to know about life expectancy around the world in 2018. This will include calculating descriptive statistics and graphing a histogram to examine the distribution of our data. R allows us to handle these tasks efficiently with a few lines of code.\n\n\n\nLoad the Data:\n\nFirst, ensure you have the dataset life_expectancy.csv in your working directory.\nLoad the data into R.\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Load the data\nlife_expectancy &lt;- read.csv(\"data/life_expectancy.csv\")\nCalculate Descriptive Statistics:\n\nCalculate the measures of central tendency (mean, median, mode) and measures of variability (range, standard deviation, variance) for life expectancy in 2018.\n\n# Filter for the year 2018\nlife_2018 &lt;- life_expectancy %&gt;% filter(Year == 2018) %&gt;% select(life_expectancy)\n\n# Calculate descriptive statistics\n\n# Summarize the results\nCreate a Histogram:\n\nPlot a histogram to visualize the distribution of life expectancy in 2018, and overlay a normal curve.\n\n# Plot the histogram with a normal curve\nggplot(life_2018, aes(x = life_expectancy)) +\n  geom_histogram(aes(y = ..density..), binwidth = 2, fill = \"skyblue\", color = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = mean_life, sd = sd_life), color = \"red\", size = 1) +\n  labs(title = \"Histogram of Life Expectancy in 2018\",\n       x = \"Life Expectancy\",\n       y = \"Density\") +\n  theme_minimal()\n\nThis process will produce a histogram displaying the distribution of life expectancy in 2018 with a superimposed normal curve for comparison.\n\n\n\nThink about what the mean, median, and mode indicate about the shape of the distribution. Is this confirmed when you look at the histogram? How does the shape of this distribution compare to the symmetrical normal distribution that is superimposed over it? The histogram helps visualize the skewness and spread of the data, providing a deeper understanding of the life expectancy distribution in 2018.\n\n\n\n\n\n\nUsing the life expectancy data set, produce a table of output showing the descriptive statistics (measures of central tendency and variability) for both years 1800 and 1934 (during the Great Depression).\nPlot histograms of life expectancy for both years. How are these distributions different? (Hint: Plot these on the same axes so that they are comparable).",
    "crumbs": [
      "Introduction",
      "R Basics"
    ]
  },
  {
    "objectID": "Chapter01_Introduction.html#installing",
    "href": "Chapter01_Introduction.html#installing",
    "title": "R Basics",
    "section": "",
    "text": "The R base package can be downloaded from the official R website. Once on the website, select the precompiled binary for your operating system, download the file, and install it. To verify that R has been successfully installed, open your command prompt (cmd) or terminal and type R to start the R console. To exit the R console, type q().\n$ R\n\nR version 4.4.0 (2024-04-24) -- \"Puppy Cup\"\nCopyright (C) 2024 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n&gt; q()",
    "crumbs": [
      "Introduction",
      "R Basics"
    ]
  },
  {
    "objectID": "Chapter01_Introduction.html#introduction-to-rstudio",
    "href": "Chapter01_Introduction.html#introduction-to-rstudio",
    "title": "R Basics",
    "section": "",
    "text": "RStudio is an essential tool for anyone working with the R programming language. It serves as an integrated development environment (IDE) that makes working with R more efficient and user-friendly. Here’s what you need to know:\n\n\nRStudio is a flexible, multifunctional open-source IDE that serves as a graphical front-end for working with R (version 3.0.1 or higher). Additionally, it supports other programming languages like Python and SQL. Key features of RStudio include a user-friendly interface, the ability to write and save reusable scripts, and easy access to imported data and created objects such as variables and functions. It provides exhaustive help on any object, offers code autocompletion, and facilitates project organization and collaboration. Users can preview plots, switch between the terminal and console, and track their operational history, making it an indispensable tool for data analysis and programming.\n\n\n\nTo install RStudio, begin by visiting the official RStudio website. Scroll down to locate the download buttons for RStudio Desktop. Select the precompiled binary appropriate for your operating system, download the file, and install it.",
    "crumbs": [
      "Introduction",
      "R Basics"
    ]
  },
  {
    "objectID": "Chapter01_Introduction.html#additional-resources",
    "href": "Chapter01_Introduction.html#additional-resources",
    "title": "R Basics",
    "section": "",
    "text": "DataCamp’s RStudio Tutorial: A comprehensive guide for beginners.\nDataquest’s Getting Started with R and RStudio: Learn key features and start programming in R.\nGitHub Pages: Introduction to RStudio: Fundamentals of RStudio for scientific projects.\nIntroduction to R and RStudio (GitBook): Best practices for organizing code using RStudio.\nRStudio for the Total Beginner: An accessible introduction to RStudio for the total beginner.\n\n\n\n\nLearning Statistics with R\nStatistical Methods in Medical Research",
    "crumbs": [
      "Introduction",
      "R Basics"
    ]
  },
  {
    "objectID": "Chapter01_Introduction.html#descriptive-statistics",
    "href": "Chapter01_Introduction.html#descriptive-statistics",
    "title": "R Basics",
    "section": "",
    "text": "Descriptive statistics is a branch of statistics that focuses on summarizing and describing the features of a dataset. It provides simple summaries about the sample and the measures, offering a way to present data in a meaningful and understandable form. The origins of descriptive statistics date back to the early days of statistical science, where it served as the foundational method for analyzing data.\nHistory:\nThe roots of descriptive statistics can be traced back to ancient civilizations, such as the Babylonians and Egyptians, who used basic statistical methods to manage agricultural data, censuses, and astronomical information. However, the formal development of descriptive statistics began in the 17th and 18th centuries. John Graunt, an English demographer, is often credited with laying the groundwork for statistical analysis through his work on mortality rates in the 1660s. His pioneering efforts in collecting and analyzing data led to the birth of modern statistics.\nIn the 18th century, the field was further advanced by the work of mathematicians like Pierre-Simon Laplace and Carl Friedrich Gauss, who developed key statistical concepts and methods. The 19th and 20th centuries saw the emergence of more sophisticated statistical techniques and the formalization of the discipline. Descriptive statistics evolved to include a variety of measures and graphical representations that remain fundamental to data analysis today.\n\nMeasures of Central Tendency:\n\nMean: The arithmetic average of a set of numbers. For example, in a dataset of test scores (80, 85, 90, 75, 95), the mean score is calculated as (80 + 85 + 90 + 75 + 95) / 5 = 85.\nMedian: The middle value in a dataset when the numbers are arranged in ascending order. In the test scores example, the median score is 85.\nMode: The most frequently occurring value in a dataset. If a dataset of test scores is (80, 85, 85, 90, 95), the mode is 85.\n\nMeasures of Dispersion:\n\nRange: The difference between the highest and lowest values in a dataset. For test scores (75, 80, 85, 90, 95), the range is 95 - 75 = 20.\nVariance: A measure of the spread of a dataset. It is the average of the squared differences from the mean. For the test scores, the variance can be calculated by finding the mean, subtracting each score from the mean, squaring the result, and then averaging those squared differences.\nStandard Deviation: The square root of the variance, representing the average amount each value in the dataset differs from the mean. It provides insight into the dataset’s overall variability.\n\nGraphical Representations:\n\nHistograms: Bar graphs that represent the frequency distribution of a dataset. They help in visualizing the shape and spread of data.\nBox Plots: Graphical representations that show the distribution of a dataset based on a five-number summary: minimum, first quartile, median, third quartile, and maximum. Box plots highlight the central tendency and variability, as well as potential outliers.\nPie Charts: Circular charts divided into sectors representing proportions of the whole. They are useful for displaying categorical data and comparing parts of a whole.\n\n\nDescriptive statistics is essential in various fields, including economics, psychology, and social sciences, where it aids in making data-driven decisions. By providing a clear summary of data through measures of central tendency, dispersion, and graphical representations, descriptive statistics helps researchers and analysts interpret and communicate their findings effectively.\n\n\nTo perform descriptive statistics, we need to assign some list of numbers to a variable. This can be done using the `c()` function, which stands for combine.\n\nmy_numbers &lt;- c(1,5,3,2)\n\nThere a few other handy ways to make numbers. We can use seq() to make a sequence. Here’s making the numbers from 1 to 100\n\none_to_one_hundred &lt;- seq(1,100,1)\n\nWe can repeat things, using rep. Here’s making 5 10s, and 25 1s:\n\nrep(10,5)\n\n[1] 10 10 10 10 10\n\nrep(1,25)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\nall_together_now &lt;- c(rep(10,5),rep(1,25)) \n\n\n\n\nLet’s work with the numbers 1 to 256. First, we’ll use the `sum()` function to add them up.\n\none_to_one_hundred &lt;- seq(1,100,1)\nsum(one_to_one_hundred)\n\n[1] 5050\n\n\n\n\n\nWe put 100 numbers into the variable one_to_one_hundred. We know how many numbers there are in there. How can we get R to tell us? We use length() for that.\n\nlength(one_to_one_hundred)\n\n[1] 100\n\n\n\n\n\n\n\nRemember the mean of some numbers is their sum, divided by the number of numbers. We can compute the mean like this:\n\nsum(one_to_one_hundred)/length(one_to_one_hundred)\n\n[1] 50.5\n\n\nOr, we could just use the mean() function like this:\n\nmean(one_to_one_hundred)\n\n[1] 50.5\n\n\n\n\n\nThe median is the number that lies exactly in the middle of a sorted list of numbers. If the list has an even number of elements, the median is calculated as the average of the two middle numbers.\n\nIf \\(n\\) is odd, \\(med(X) = X_{(n+1)/2}\\)\nIf \\(n\\) is even, \\(med(X) = \\frac{X_{n/2} + X_{(n+2) + 1}}{2}\\)\n\nYou can use the median() function to find it. For example, in a list of three numbers, the middle number is 2, so the median is 2.\n\nmedian(c(1,2,3))\n\n[1] 2\n\n\n\n\n\nR does not have a built-in function for calculating the mode. You will need to write one yourself. Here is an example of how to create a mode function and use it. Remember, the mode is the number that appears most frequently in a dataset. In the example below, 1 occurs the most often, so the mode is 1.\n\nmode &lt;- function(x) {\n  ux &lt;- unique(x)\n  ux[which.max(tabulate(match(x, ux)))]\n}\n\nmode(c(1,1,1,1,1,1,1,2,3,4))\n\n[1] 1\n\n\n\n\n\n\nWe often want to understand how much the numbers vary. To describe this variability, we look at descriptive statistics such as range, variance, the standard deviation, and others.\nFirst, let’s remind ourselves what variation looks like (it refers to differences among numbers). We’ll sample 1000 numbers from a normal distribution with a mean of 101 and a standard deviation of 35. Then, we’ll create a histogram to visualize the variation around the mean of 10.\n\nsample_numbers &lt;- rnorm(1000,101,35)\nhist(sample_numbers)\n\n\n\n\n\n\n\n\n\n\nThe range is the minimum and maximum values in the set, we use the range function.\n\nrange(sample_numbers)\n\n[1]  -7.461576 207.119929\n\n\n\n\n\nWe can find the sample variance using var().\n\nvar(sample_numbers)\n\n[1] 1296.951\n\n\n\n\n\nWe find the sample standard deviation us sd().\n\nsd(sample_numbers)\n\n[1] 36.01321\n\n\nRemember that the standard deviation is just the square root of the variance, see:\n\nsqrt(var(sample_numbers))\n\n[1] 36.01321\n\n\n\n\n\n\nsample_numbers &lt;- rnorm(1000,101,35)\n\nsum(sample_numbers)\n\n[1] 101370.6\n\nlength(sample_numbers)\n\n[1] 1000\n\nmean(sample_numbers)\n\n[1] 101.3706\n\nmedian(sample_numbers)\n\n[1] 101.2754\n\nmode(sample_numbers)\n\n[1] 48.96654\n\nrange(sample_numbers)\n\n[1] -42.53119 227.15131\n\nvar(sample_numbers)\n\n[1] 1286.185\n\nsd(sample_numbers)\n\n[1] 35.86342\n\n\n\n\n\n\nIn some instances, a single variable may contain a number of values, which can be analyzed using the aforementioned functions to obtain descriptive statistics. In contrast, in the majority of cases encountered in this course, a data frame comprising multiple numerical values representing distinct conditions will be utilized. In such instances, it is imperative to identify descriptive statistics for each set of values within each condition.\nFortunately, the R programming language is highly adept at performing this task in a single operation. To illustrate this concept, consider the following example. A data frame containing 10 numbers for each condition will be created. There are 10 conditions, each labeled A, B, C, D, E, F, G, H, I, and J.\n\nscores &lt;- rnorm(100,10,5)\nconditions &lt;- rep(c(\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\"), each =10)\nmy_df &lt;- data.frame(conditions,scores)\n\nA review of the my_df data frame reveals that it contains 100 rows. Each row represents a distinct condition, with a label in the conditions column and 10 scores for that condition in the scores column. One might inquire as to the mean of the scores in each condition. In order to obtain the mean of the scores for each condition, one must find the mean of 10 scores.\nThe slow way to do it would be like this:\n\nmean(my_df[my_df$conditions==\"A\",]$scores)\n\n[1] 8.749205\n\nmean(my_df[my_df$conditions==\"B\",]$scores)\n\n[1] 10.85871\n\nmean(my_df[my_df$conditions==\"C\",]$scores)\n\n[1] 7.650824\n\n# and then keep going\n\nIt is evident that no individual or entity is willing to assume the responsibility of performing this task. It is therefore prudent to encapsulate this functionality, and R provides us with the necessary tools to automate this functionality.\n\n\nWe can easily do everything all at once using the group_by and summarise function from the dplyr package. Just watch\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nmy_df %&gt;%\n  group_by(conditions) %&gt;%\n  summarise(means = mean(scores))\n\n# A tibble: 10 × 2\n   conditions means\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 A           8.75\n 2 B          10.9 \n 3 C           7.65\n 4 D          10.0 \n 5 E          12.2 \n 6 F           9.27\n 7 G          11.3 \n 8 H          10.2 \n 9 I           8.76\n10 J          12.4 \n\n\nA few points require further consideration. Firstly, the printout of this was of an inferior quality. A solution to this issue is to create a new variable containing the results of the code and then use the knitr::kable function to print the variable in a more aesthetically pleasing manner when the document is compiled.\n\nsummary_df &lt;- my_df %&gt;%\n               group_by(conditions) %&gt;%\n               summarise(means = mean(scores))\n\nknitr::kable(summary_df)\n\n\n\n\nconditions\nmeans\n\n\n\n\nA\n8.749205\n\n\nB\n10.858707\n\n\nC\n7.650824\n\n\nD\n10.022154\n\n\nE\n12.234878\n\n\nF\n9.266721\n\n\nG\n11.306428\n\n\nH\n10.228266\n\n\nI\n8.759115\n\n\nJ\n12.424293\n\n\n\n\n\n\n\n\nThe most advantageous aspect of the dplyr method is that it permits the addition of multiple functions, resulting in the generation of multiple summary statistics in a unified format. To illustrate this, consider the calculation of the standard deviation:\n\nsummary_df &lt;- my_df %&gt;%\n               group_by(conditions) %&gt;%\n               summarise(means = mean(scores),\n                         sds = sd(scores))\n\nknitr::kable(summary_df)\n\n\n\n\nconditions\nmeans\nsds\n\n\n\n\nA\n8.749205\n3.809475\n\n\nB\n10.858707\n4.655481\n\n\nC\n7.650824\n4.776866\n\n\nD\n10.022154\n5.564239\n\n\nE\n12.234878\n3.934642\n\n\nF\n9.266721\n4.074786\n\n\nG\n11.306428\n6.359828\n\n\nH\n10.228266\n5.714836\n\n\nI\n8.759115\n7.025043\n\n\nJ\n12.424293\n6.270404\n\n\n\n\n\nFurthermore, the minimum and maximum values will be included.\n\nsummary_df &lt;- my_df %&gt;%\n               group_by(conditions) %&gt;%\n               summarise(means = mean(scores),\n                         sds = sd(scores),\n                         min = min(scores),\n                         max = max(scores))\n\nknitr::kable(summary_df)\n\n\n\n\nconditions\nmeans\nsds\nmin\nmax\n\n\n\n\nA\n8.749205\n3.809475\n2.6909396\n14.36260\n\n\nB\n10.858707\n4.655481\n4.3610864\n18.34482\n\n\nC\n7.650824\n4.776866\n1.2825037\n19.33234\n\n\nD\n10.022154\n5.564239\n3.2911257\n17.71844\n\n\nE\n12.234878\n3.934642\n6.2258202\n18.29770\n\n\nF\n9.266721\n4.074786\n3.3725043\n17.87850\n\n\nG\n11.306428\n6.359828\n2.4939802\n23.04869\n\n\nH\n10.228266\n5.714836\n0.0305871\n17.12949\n\n\nI\n8.759115\n7.025043\n-4.1064345\n18.98884\n\n\nJ\n12.424293\n6.270404\n1.8025658\n20.80197\n\n\n\n\n\n\n\n\n\nHaving established the methodology for obtaining descriptive statistics from R, we may now proceed to apply this methodology to a real data set. We will now proceed to inquire about the gapminder data.\n\nlibrary(gapminder)\ngapminder_df &lt;- gapminder\n\nNote: The code will only function properly if the gapminder package has been installed. It is imperative that the user is connected to the internet prior to commencing the installation process. To do so, navigate to the Packages tab, located in the bottom right panel, and select the option to install. Subsequently, a search for the gapminder package should be conducted, after which it should be selected and installed.\n\n\nCopy the code from the last part of descriptives using dplyr, then change the names like this:\n\nsummary_df &lt;- gapminder_df %&gt;%\n               group_by(continent) %&gt;%\n               summarise(means = mean(lifeExp),\n                         sds = sd(lifeExp),\n                         min = min(lifeExp),\n                         max = max(lifeExp))\n\nknitr::kable(summary_df)\n\n\n\n\ncontinent\nmeans\nsds\nmin\nmax\n\n\n\n\nAfrica\n48.86533\n9.150210\n23.599\n76.442\n\n\nAmericas\n64.65874\n9.345088\n37.579\n80.653\n\n\nAsia\n60.06490\n11.864532\n28.801\n82.603\n\n\nEurope\n71.90369\n5.433178\n43.585\n81.757\n\n\nOceania\n74.32621\n3.795611\n69.120\n81.235\n\n\n\n\n\n\n\n\n\nComplete the generalization exercise described in your R Markdown document for this lab.\n\nWhat is the mean, standard deviation, minimum and maximum life expectancy for all the gapminder data (across all the years and countries). Hint: do not use group_by\nWhat is the mean, standard deviation, minimum and maximum life expectancy for all of the continents in 2007, the most recent year in the dataset. Hint: add another pipe using filter(year==2007) %&gt;%\n\n\n\n\nComplete the writing assignment described in your R Markdown document for this lab. When you have finished everything. Knit the document and hand in your stuff (you can submit your .RMD file to blackboard if it does not knit.)\nYour writing assignment is to answer these questions in full sentences using simple plain langauge:\n\nDefine the mode.\nExplain what would need to happen in order for a set of numbers to have two modes\nDefine the median\nDefine the mean\nDefine the range\nWhen calculating the standard deviation, explain what the difference scores represent\nExplain why the difference scores are squared when calculating the standard deviation\nIf one set of numbers had a standard deviation of 5, and another had a standard deviation of 10, which set of numbers would have greater variance, explain why.\n\n\n\n\nNow let’s use a real dataset to calculate the same measures of central tendency and variability as in the previous example, but with the addition of a histogram to visualize the distribution and relate back to the descriptive statistics. Here is a link to the life expectancy dataset we used for our graphing tutorial. It is named life_expectancy.csv.\nSuppose we wanted to know about life expectancy around the world in 2018. This will include calculating descriptive statistics and graphing a histogram to examine the distribution of our data. R allows us to handle these tasks efficiently with a few lines of code.\n\n\n\nLoad the Data:\n\nFirst, ensure you have the dataset life_expectancy.csv in your working directory.\nLoad the data into R.\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Load the data\nlife_expectancy &lt;- read.csv(\"data/life_expectancy.csv\")\nCalculate Descriptive Statistics:\n\nCalculate the measures of central tendency (mean, median, mode) and measures of variability (range, standard deviation, variance) for life expectancy in 2018.\n\n# Filter for the year 2018\nlife_2018 &lt;- life_expectancy %&gt;% filter(Year == 2018) %&gt;% select(life_expectancy)\n\n# Calculate descriptive statistics\n\n# Summarize the results\nCreate a Histogram:\n\nPlot a histogram to visualize the distribution of life expectancy in 2018, and overlay a normal curve.\n\n# Plot the histogram with a normal curve\nggplot(life_2018, aes(x = life_expectancy)) +\n  geom_histogram(aes(y = ..density..), binwidth = 2, fill = \"skyblue\", color = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = mean_life, sd = sd_life), color = \"red\", size = 1) +\n  labs(title = \"Histogram of Life Expectancy in 2018\",\n       x = \"Life Expectancy\",\n       y = \"Density\") +\n  theme_minimal()\n\nThis process will produce a histogram displaying the distribution of life expectancy in 2018 with a superimposed normal curve for comparison.\n\n\n\nThink about what the mean, median, and mode indicate about the shape of the distribution. Is this confirmed when you look at the histogram? How does the shape of this distribution compare to the symmetrical normal distribution that is superimposed over it? The histogram helps visualize the skewness and spread of the data, providing a deeper understanding of the life expectancy distribution in 2018.\n\n\n\n\n\n\nUsing the life expectancy data set, produce a table of output showing the descriptive statistics (measures of central tendency and variability) for both years 1800 and 1934 (during the Great Depression).\nPlot histograms of life expectancy for both years. How are these distributions different? (Hint: Plot these on the same axes so that they are comparable).",
    "crumbs": [
      "Introduction",
      "R Basics"
    ]
  },
  {
    "objectID": "Chaper05_Counting.html",
    "href": "Chaper05_Counting.html",
    "title": "Counting",
    "section": "",
    "text": "Alignment and mapping algorithms are crucial in bioinformatics for sequence alignment and genome assembly. Here are some of the most commonly used algorithms:\n\n\nThe Smith-Waterman algorithm(Smith 1981) is a classic dynamic programming algorithm used for local sequence alignment. It identifies similar regions between two strings or nucleotide or protein sequences.\n\n\n\nThe Needleman-Wunsch algorithm(Needleman 1970) is also a dynamic programming algorithm, but it’s used for global sequence alignment. It finds the best alignment (including gaps) between two sequences.\n\n\n\nThe Burrows-Wheeler Transform (BWT)(Burrows 1994) is an algorithm that is used in data compression, notably in bzip2 and the sequence aligner Bowtie. It transforms a string into a more compressible format, which is useful for both data compression and bioinformatics.\n\n\n\nBLAST(Altschul 1990) is an algorithm for comparing primary biological sequence information, such as the amino-acid sequences of proteins or the nucleotides of DNA and/or RNA sequences. It’s a very fast sequence alignment algorithm primarily used to search sequence databases for optimal local alignments to a query."
  },
  {
    "objectID": "Chaper05_Counting.html#smith-waterman-algorithm",
    "href": "Chaper05_Counting.html#smith-waterman-algorithm",
    "title": "Counting",
    "section": "",
    "text": "The Smith-Waterman algorithm(Smith 1981) is a classic dynamic programming algorithm used for local sequence alignment. It identifies similar regions between two strings or nucleotide or protein sequences."
  },
  {
    "objectID": "Chaper05_Counting.html#needleman-wunsch-algorithm",
    "href": "Chaper05_Counting.html#needleman-wunsch-algorithm",
    "title": "Counting",
    "section": "",
    "text": "The Needleman-Wunsch algorithm(Needleman 1970) is also a dynamic programming algorithm, but it’s used for global sequence alignment. It finds the best alignment (including gaps) between two sequences."
  },
  {
    "objectID": "Chaper05_Counting.html#burrows-wheeler-transform-bwt",
    "href": "Chaper05_Counting.html#burrows-wheeler-transform-bwt",
    "title": "Counting",
    "section": "",
    "text": "The Burrows-Wheeler Transform (BWT)(Burrows 1994) is an algorithm that is used in data compression, notably in bzip2 and the sequence aligner Bowtie. It transforms a string into a more compressible format, which is useful for both data compression and bioinformatics."
  },
  {
    "objectID": "Chaper05_Counting.html#blast-basic-local-alignment-search-tool",
    "href": "Chaper05_Counting.html#blast-basic-local-alignment-search-tool",
    "title": "Counting",
    "section": "",
    "text": "BLAST(Altschul 1990) is an algorithm for comparing primary biological sequence information, such as the amino-acid sequences of proteins or the nucleotides of DNA and/or RNA sequences. It’s a very fast sequence alignment algorithm primarily used to search sequence databases for optimal local alignments to a query."
  },
  {
    "objectID": "Chaper05_Counting.html#microarray",
    "href": "Chaper05_Counting.html#microarray",
    "title": "Counting",
    "section": "Microarray",
    "text": "Microarray\nMicroarrays are a well-established technology that allows for the measurement of expression levels of thousands of genes simultaneously (Schena 1995). They use probes that hybridize to specific target sequences present in the sample. The intensity of the signal from each spot on the array corresponds to the quantity of the target sequence in the sample."
  },
  {
    "objectID": "Chaper05_Counting.html#rna-sequencing",
    "href": "Chaper05_Counting.html#rna-sequencing",
    "title": "Counting",
    "section": "RNA Sequencing",
    "text": "RNA Sequencing\nRNA sequencing (RNA-Seq) is a more recent technology that provides a far more precise measurement of levels of transcripts and their isoforms (Wang 2009). Unlike microarrays, RNA-Seq does not require prior knowledge of the sequences and can therefore be used to discover novel transcripts."
  },
  {
    "objectID": "Chaper05_Counting.html#differences",
    "href": "Chaper05_Counting.html#differences",
    "title": "Counting",
    "section": "Differences",
    "text": "Differences\nThe main differences between microarrays and RNA-Seq include:\n\nResolution: RNA-Seq has a higher resolution than microarrays. It can detect low abundance transcripts, alternative splicing, and mutations (Wang 2009).\nDynamic Range: RNA-Seq has a larger dynamic range than microarrays, allowing for the detection of more differentially expressed genes with higher fold change (Marioni 2008).\nCost: Microarrays are generally less expensive than RNA-Seq."
  },
  {
    "objectID": "Chaper05_Counting.html#application-on-gene-differential-analysis",
    "href": "Chaper05_Counting.html#application-on-gene-differential-analysis",
    "title": "Counting",
    "section": "Application on Gene Differential Analysis",
    "text": "Application on Gene Differential Analysis\nBoth microarrays and RNA-Seq can be used for gene differential analysis, which involves finding differences in gene expression between different groups of samples. RNA-Seq is often preferred for this application due to its higher resolution and larger dynamic range (Marioni 2008)."
  },
  {
    "objectID": "Chaper05_Counting.html#hypothesis-testing",
    "href": "Chaper05_Counting.html#hypothesis-testing",
    "title": "Counting",
    "section": "1. Hypothesis Testing",
    "text": "1. Hypothesis Testing\nHypothesis testing is a statistical method used to make inferences or draw conclusions about a population based on a sample of data. In the context of differential expression analysis, hypothesis testing is used to determine whether the difference in gene expression between two conditions is statistically significant (Lehmann 2006)."
  },
  {
    "objectID": "Chaper05_Counting.html#multiple-testing-correction",
    "href": "Chaper05_Counting.html#multiple-testing-correction",
    "title": "Counting",
    "section": "2. Multiple Testing Correction",
    "text": "2. Multiple Testing Correction\nWhen performing multiple hypothesis tests, the chance of obtaining false positives increases. Multiple testing correction techniques, such as the Bonferroni correction or the Benjamini-Hochberg procedure, are used to control the false positive rate (Benjamini 1995)."
  },
  {
    "objectID": "Chaper05_Counting.html#regression-analysis",
    "href": "Chaper05_Counting.html#regression-analysis",
    "title": "Counting",
    "section": "3. Regression Analysis",
    "text": "3. Regression Analysis\nRegression analysis is used to model the relationship between a dependent variable (gene expression level) and one or more independent variables (experimental conditions). Linear regression and logistic regression are commonly used methods in differential expression analysis (Draper 2014)."
  },
  {
    "objectID": "Chaper05_Counting.html#bayesian-statistics",
    "href": "Chaper05_Counting.html#bayesian-statistics",
    "title": "Counting",
    "section": "4. Bayesian Statistics",
    "text": "4. Bayesian Statistics\nBayesian statistics is a statistical paradigm that deals with the updating of probabilities based on observed data. Bayesian methods, such as Bayesian hierarchical models, are increasingly being used in differential expression analysis to share information across genes (Kruschke 2015)."
  },
  {
    "objectID": "Chaper05_Counting.html#downloading-genome-files",
    "href": "Chaper05_Counting.html#downloading-genome-files",
    "title": "Counting",
    "section": "Downloading Genome Files",
    "text": "Downloading Genome Files\nWe have provided the index files for chromosome 1 for the mouse genome build mm10 for this lecture in order to save time on building the index. However, full genome fasta files for a number of different genomes are available to download from the UCSC genome browser, see here; from NCBI: here; or from ENSEMBL: here."
  },
  {
    "objectID": "Chaper05_Counting.html#introduction-and-data-import",
    "href": "Chaper05_Counting.html#introduction-and-data-import",
    "title": "Counting",
    "section": "Introduction and Data Import",
    "text": "Introduction and Data Import\nFor the purpose of this lecture, we are going to be working with a small part of the mouse reference genome (chromosome 1) to show how to do read alignment and counting using R. Mapping reads to the genome is a very important step, and many different mappers/aligners are available, such as bowtie (Langmead and Salzberg 2012), topHat (Trapnell, Pachter, and Salzberg 2009), STAR (Dobin et al. 2013) and Rsubread (Liao, Smyth, and Shi 2013). Rsubread is the only mappers/aligner that can run in R. Most alignment tools are run in a linux environment, and they are very computationally intensive. Most mapping tasks require larger computers than an average laptop, so usually read mapping is done on a server in a linux-like environment. Here we are only going to be mapping 1000 reads from each sample from our mouse lactation dataset (Fu et al. 2015), and we will only be mapping to chromosome 1.\nFirst, let’s load the Rsubread package into R.\n\n# Load necessary library\n# To install package, enter in console\"\n# if (!require(\"BiocManager\", quietly = TRUE))\n#    install.packages(\"BiocManager\")\n# BiocManager::install(\"Biostrings\")\nlibrary(Rsubread)\n\nEarlier we put all the sequencing read data (.fastq.gz files) in the data directory. Now we need to find them in order to tell the Rsubread aligner which files to look at. We can search for all .fastq.gz files in the data directory using the list.files command. The pattern argument takes a regular expression. In this case we are using the $ to mean the end of the string, so we will only get files that end in “.fastq.gz”\n\nfastq.files &lt;- list.files(path = \"./data\", pattern = \".fastq.gz$\", full.names = TRUE)\nfastq.files"
  },
  {
    "objectID": "Chaper05_Counting.html#alignment-with-r",
    "href": "Chaper05_Counting.html#alignment-with-r",
    "title": "Counting",
    "section": "Alignment with R",
    "text": "Alignment with R\n\nBuild the Index\nRead sequences are stored in compressed (gzipped) FASTQ files. Before the differential expression analysis can proceed, these reads must be aligned to the mouse genome and counted into annotated genes. This can be achieved with functions in the Rsubread package.\nThe first step in performing the alignment is to build an index. In order to build an index you need to have the fasta file (.fa), which can be downloaded from the UCSC genome browser. Here we are building the index just for chromosome 1. This may take several minutes to run. Building the full index using the whole mouse genome usually takes about 30 minutes to an hr on a server. We won’t be building the index in the lecture due to time constraints, we have provided the index files for you. The command below assumes the chr1 genome information for mm10 is stored in the “chr1.fa” file.\n\n# We have provided the index files. You do not need to run command below.\n# buildindex(basename=\"data/chr1_mm10\", reference=\"chr1.fa\")\n\nThe above command will generate the index files in the working directory. In this example, the prefix for the index files is chr1_mm10. You can see the additional files generated using the list.files command, which will list every file in your current working directory.\n\nlist.files(\"data/\")\n\n\n\nAligning Reads to Chromosome 1 of Reference Genome\nNow that we have generated our index, we can align our reads using the align command. There are often numerous mapping parameters that we can specify, but usually the default mapping parameters for the align function are fine. If we had paired end data, we would specify the second read file/s using the readfile2 argument. Our mouse data comprises 100bp single end reads.\nWe can specify the output files, or we can let Rsubread choose the output file names for us. The default output file name is the filename with “.subread.BAM” added at the end.\nNow we can align our 12 fastq.gz files using the align command.\n\nalign(index=\"data/chr1_mm10\", readfile1 = fastq.files)\n\nThis will align each of the 12 samples one after the other. As we’re only using a subset of 1000 reads per sample, aligning should just take a minute or so for each sample. To run the full samples from this dataset would take several hours per sample. The BAM files are saved in the working directory.\nTo see how many parameters you can change try the args function:\n\nargs(align)\n\nIn this example we have kept many of the default settings, which have been optimised to work well under a variety of situations. The default setting for align is that it only keeps reads that uniquely map to the reference genome. For testing differential expression of genes, this is what we want, as the reads are unambigously assigned to one place in the genome, allowing for easier interpretation of the results. Understanding all the different parameters you can change involves doing a lot of reading about the aligner that you are using, and can take a lot of time to understand! Today we won’t be going into the details of the parameters you can change, but you can get more information from looking at the help:\n\n?align\n\nWe can get a summary of the proportion of reads that mapped to the reference genome using the propmapped function.\n\nbam.files &lt;- list.files(path = \"./data\", pattern = \".BAM$\", full.names = TRUE)\nbam.files\n\n\nprops &lt;- propmapped(files=bam.files)\nprops\n\n\n\nExercise 1\n\nObjetive\nThe objective of this exercise is to explore the impact of alignment parameters on the mapping of sequencing reads. Specifically, we aim to:\n\nInvestigate the effect of allowing multi-mapping reads: By setting unique = FALSE, we allow reads that map to multiple locations in the genome. This can be particularly useful for understanding the behavior of repetitive sequences or regions of high sequence similarity.\nExamine the influence of reporting multiple “best” locations: By setting nBestLocations = 6, we allow up to six of the best matching locations for each read to be reported. This can provide a more comprehensive view of potential mapping locations for each read.\nTry aligning the fastq files allowing multi-mapping reads (set unique = FALSE), and allowing for up to 6 “best” locations to be reported (nBestLocations = 6). Specify the output file names (bam.files.multi) by substituting “.fastq.gz” with “.multi.bam” so we don’t overwrite our unique alignment bam files.\nLook at the proportion of reads mapped and see if we get any more reads mapping by specifying a less stringent criteria."
  },
  {
    "objectID": "Chaper05_Counting.html#quality-control",
    "href": "Chaper05_Counting.html#quality-control",
    "title": "Counting",
    "section": "Quality Control",
    "text": "Quality Control\nQuality control (QC) is a critical step in the RNA sequencing workflow. It involves assessing the quality of the raw sequencing data, which are typically stored in FASTQ files. Each FASTQ file contains a list of reads along with their corresponding quality scores. These scores indicate the confidence of each base call. Low-quality reads, adapter sequences, and contaminants can be identified at this stage and filtered out before downstream analysis. It’s important to note that QC should be an iterative process, performed before and after each major step in the RNA-Seq data analysis pipeline.\nWe can have a look at the quality scores associated with each base that has been called by the sequencing machine using the qualityScores function in Rsubread.\nLet’s first extract quality scores for 100 reads for the file “SRR1552450.fastq.gz”.\n\n# Extract quality scores\nqs &lt;- qualityScores(filename=\"data/SRR1552450.fastq.gz\",nreads=100)\n# Check dimension of qs\ndim(qs)\n# Check first few elements of qs with head\nhead(qs)\n\nA quality score of 30 corresponds to a 1 in 1000 chance of an incorrect base call. (A quality score of 10 is a 1 in 10 chance of an incorrect base call.) To look at the overall distribution of quality scores across the 100 reads, we can look at a boxplot\n\nboxplot(qs)\n\n\nExercise\n\nExtract quality scores for SRR1552451.fastq.gz for 50 reads.\nPlot a boxplot of the quality scores for SRR1552451.fastq.gz.”"
  },
  {
    "objectID": "Chaper05_Counting.html#counting-1",
    "href": "Chaper05_Counting.html#counting-1",
    "title": "Counting",
    "section": "Counting",
    "text": "Counting\nOnce we have determined where each read originates in the genome, we need to summarize this information across genes or exons. The alignment process generates a set of BAM files, with each file containing the read alignments for each library. In the BAM file, there is a chromosomal location for every read that mapped uniquely. The mapped reads can be counted across mouse genes using the featureCounts function. featureCounts contains built-in annotation for mouse (mm9, mm10) and human (hg19) genome assemblies (NCBI refseq annotation) (Liao, Smyth, and Shi 2014).\nThe code below uses the exon intervals defined in the NCBI refseq annotation of the mm10 genome. Reads that map to exons of genes are added together to obtain the count for each gene, with some care taken with reads that span exon-exon boundaries. featureCounts takes all the BAM files as input, and outputs an object which includes the count matrix. Each sample is a separate column, each row is a gene.\n\nfc &lt;- featureCounts(bam.files, annot.inbuilt=\"mm10\")\n\nThe names() function in R is a built-in function that gets or sets the names of an object. In R, many types of objects can have names, including vectors, lists, data frames, and more.\n\nnames(fc)\n\nThe statistics of the read mapping can be seen with fc$stats. This reports the numbers of unassigned reads and the reasons why they are not assigned (eg. ambiguity, multi-mapping, secondary alignment, mapping quality, fragment length, chimera, read duplicate, non-junction and so on), in addition to the number of successfully assigned reads for each library. See subread documentation (‘Program output’ section). (We know the real reason why the majority of the reads aren’t mapping - they’re not from chr 1!)\n\n## Take a look at the feature counts stats\nfc$stat\n\nThe counts for the samples are stored in fc$counts.\n\n## Take a look at the dimensions to see the number of genes\ndim(fc$counts)\n## Take a look at the first lines\nhead(fc$counts)\n\nThe row names of the fc$counts matrix represent the Entrez gene identifiers(Maglott et al. 2007) for each gene and the column names are the output filenames from calling the align function. The annotation slot shows the annotation information that featureCounts used to summarise reads over genes.\n\nhead(fc$annotation)\n\n\nExercise\n\nRedo the counting over the exons, rather than the genes (specify useMetaFeatures = FALSE). Use the bam files generated doing alignment reporting only unique reads, and call the featureCounts object fc.exon. Check the dimension of the counts slot to see how much larger it is.\nUsing your “.multi.bam” files, redo the counting over genes, allowing for multimapping reads (specify countMultiMappingReads = TRUE), calling the object fc.multi. Check the stats."
  }
]